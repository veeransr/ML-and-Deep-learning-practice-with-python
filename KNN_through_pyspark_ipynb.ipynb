{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 with Spark",
      "language": "python3",
      "name": "python36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "KNN through pyspark. ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veeransr/ML-and-Deep-learning-practice-with-python/blob/master/KNN_through_pyspark_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGU9Nlr-rry1",
        "colab_type": "text"
      },
      "source": [
        "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
        "\n",
        "In case you are facing issues, please read the following two documents first:\n",
        "\n",
        "https://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n",
        "\n",
        "https://github.com/IBM/skillsnetwork/wiki/FAQ\n",
        "\n",
        "Then, please feel free to ask:\n",
        "\n",
        "https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n",
        "\n",
        "Please make sure to follow the guidelines before asking a question:\n",
        "\n",
        "https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n",
        "\n",
        "\n",
        "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrdPY-jr5jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ipkfP2DsCFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iDRVkBQsErZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l49NSLSPsN4a",
        "colab_type": "code",
        "outputId": "156bf63e-244c-4608-f6f2-f7581836e7c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 61kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 48.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=18c5783029295b4c52693d22b39d082cda2f8b3e610495c39de4b69598907d27\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggASSG_msFKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.util import MLUtils\n",
        "import numpy as np\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "import pyspark.sql.functions as f\n",
        "import pyspark.sql.types\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = SparkContext.getOrCreate()\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "gl-4CqDlrrzH",
        "colab_type": "text"
      },
      "source": [
        "Welcome to exercise two of week three of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise we’ll work on clustering.\n",
        "\n",
        "Let’s create our DataFrame again:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pAB8H1vsnL_",
        "colab_type": "code",
        "outputId": "64d6a8d1-35e6-4dcf-fe81-08c72904200e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-2.4.5-bin-hadoop2.7\tspark-2.4.5-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgZG2KHyrrzI",
        "colab_type": "code",
        "outputId": "77b1a28b-982a-40a9-c83c-d6e57ed5935a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# delete files from previous runs\n",
        "!rm -f hmp.parquet*\n",
        "\n",
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n",
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-27 19:53:49--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
            "--2020-05-27 19:53:50--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
            "--2020-05-27 19:53:50--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 932997 (911K) [application/octet-stream]\n",
            "Saving to: ‘hmp.parquet’\n",
            "\n",
            "hmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-05-27 19:53:51 (9.57 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsgQqUehswSo",
        "colab_type": "code",
        "outputId": "3a279506-3ef3-4396-b88c-9ae6f4be5d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+-----------------+-----------------+--------------------+-----------+\n",
            "|summary|                 x|                y|                z|              source|      class|\n",
            "+-------+------------------+-----------------+-----------------+--------------------+-----------+\n",
            "|  count|            446529|           446529|           446529|              446529|     446529|\n",
            "|   mean|24.671555486877672|38.21949078335338|41.83984242904716|                null|       null|\n",
            "| stddev| 12.15759300618102|7.690688487231707|8.303435899413733|                null|       null|\n",
            "|    min|                 0|                0|                0|Accelerometer-201...|Brush_teeth|\n",
            "|    max|                63|               63|               63|Accelerometer-201...|       Walk|\n",
            "+-------+------------------+-----------------+-----------------+--------------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FGVBarArrzL",
        "colab_type": "text"
      },
      "source": [
        "Let’s reuse our feature engineering pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSXGUnX0rrzM",
        "colab_type": "code",
        "outputId": "2a0ba07a-57cd-48f0-e657-840d6925eac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
        "                                  outputCol=\"features\")\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
        "model = pipeline.fit(df)\n",
        "prediction = model.transform(df)\n",
        "prediction.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
            "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n",
            "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
            "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
            "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n",
            "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n",
            "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n",
            "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n",
            "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n",
            "| 21| 51| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n",
            "| 20| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
            "| 20| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n",
            "| 18| 49| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n",
            "| 19| 48| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n",
            "| 16| 53| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n",
            "| 18| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n",
            "| 18| 51| 32|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n",
            "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF_0MNSBrrzQ",
        "colab_type": "text"
      },
      "source": [
        "Now let’s create a new pipeline for kmeans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1QXi7j0rrzR",
        "colab_type": "code",
        "outputId": "65ffef23-28c4-418c-a02e-f6d80a2c7983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "model = pipeline.fit(df)\n",
        "predictions = model.transform(df)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.41244594513295846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ONMIMjmrrzU",
        "colab_type": "text"
      },
      "source": [
        "We have 14 different movement patterns in the dataset, so setting K of KMeans to 14 is a good idea. But please experiment with different values for K, do you find a sweet spot? The closer Silhouette gets to 1, the better.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Silhouette_(clustering)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyJLoz8ctZpc",
        "colab_type": "code",
        "outputId": "216fc034-54d5-4900-87e9-bd0cb8292c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for i in range (2,20):\n",
        "  kmeans = KMeans(featuresCol=\"features\").setK(i).setSeed(1)\n",
        "  pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "  model = pipeline.fit(df)\n",
        "  predictions = model.transform(df)\n",
        "  evaluator = ClusteringEvaluator()\n",
        "  silhouette = evaluator.evaluate(predictions)\n",
        "  print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.6875664014387497\n",
            "Silhouette with squared euclidean distance = 0.6147915951361759\n",
            "Silhouette with squared euclidean distance = 0.6333227654128869\n",
            "Silhouette with squared euclidean distance = 0.5937447997439024\n",
            "Silhouette with squared euclidean distance = 0.592463658820136\n",
            "Silhouette with squared euclidean distance = 0.5484627422401509\n",
            "Silhouette with squared euclidean distance = 0.46686489256383346\n",
            "Silhouette with squared euclidean distance = 0.48034893889849645\n",
            "Silhouette with squared euclidean distance = 0.47370428136987536\n",
            "Silhouette with squared euclidean distance = 0.4819049717562352\n",
            "Silhouette with squared euclidean distance = 0.40964155503229643\n",
            "Silhouette with squared euclidean distance = 0.4153293521373778\n",
            "Silhouette with squared euclidean distance = 0.41244594513295846\n",
            "Silhouette with squared euclidean distance = 0.41771495579360896\n",
            "Silhouette with squared euclidean distance = 0.39594610810727193\n",
            "Silhouette with squared euclidean distance = 0.40512075095291467\n",
            "Silhouette with squared euclidean distance = 0.4058090075137995\n",
            "Silhouette with squared euclidean distance = 0.3794290531790819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nquorTSvrrzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# please change the pipeline the check performance for different K, feel free to use a loop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesNINBfrrzY",
        "colab_type": "text"
      },
      "source": [
        "Now please extend the pipeline to work on the normalized features. You need to tell KMeans to use the normalized feature column and change the pipeline in order to contain the normalizer stage as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Ysw7pgxEOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  kmeans = KMeans(featuresCol=\"features\").setK(i).setSeed(1)\n",
        "  pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "  model = pipeline.fit(df)\n",
        "  predictions = model.transform(df)\n",
        "  evaluator = ClusteringEvaluator()\n",
        "  silhouette = evaluator.evaluate(predictions)\n",
        "  print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-y3XfgrrzZ",
        "colab_type": "code",
        "outputId": "eaadca98-7c5e-44b7-968d-868e4e6a9468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
        "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer, kmeans])\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "predictions = model.transform(df)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.41244594513295846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtq1Ouhrrzc",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, inflating the dataset helps, here we multiply x by 10, let’s see if the performance inceases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E7FACH9rrzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "df_denormalized = df.select([col('*'),(col('x')*10)]).drop('x').withColumnRenamed('(x * 10)','x')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFCBERPBrrzi",
        "colab_type": "code",
        "outputId": "ad46bce1-3f17-41eb-cc55-f0ae29b14539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "model = pipeline.fit(df_denormalized)\n",
        "predictions = model.transform(df_denormalized)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.5709023393004293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugBe5jOUrrzm",
        "colab_type": "text"
      },
      "source": [
        "Apache SparkML can be used to try many different algorithms and parametrizations using the same pipeline. Please change the code below to use GaussianMixture over KMeans. Please use the following link for your reference.\n",
        "\n",
        "https://spark.apache.org/docs/latest/ml-clustering.html#gaussian-mixture-model-gmm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JZT-UvDrrzm",
        "colab_type": "code",
        "outputId": "bffb485c-755c-4273-aaa6-7e9ba1aba5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.clustering import GaussianMixture\n",
        "gmm = GaussianMixture().setK(2).setSeed(538009335)\n",
        "\n",
        "pipeline = Pipeline(stages=[vectorAssembler, gmm])\n",
        "\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "predictions = model.transform(df)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.34878643017699656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LctZv5M1ydCa",
        "colab_type": "text"
      },
      "source": [
        "GaussianMixture resulted in better distane\n",
        "\n",
        "\n",
        "Need to check by plotting"
      ]
    }
  ]
}